{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # XGBoost Training Refactored\n",
    "\n",
    "\n",
    "\n",
    " This notebook has been refactored to utilize utility functions from `utils.py` for better code organization and maintainability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'WandbCallback' from 'optuna.integration' (unknown location)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 16\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mxgboost\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mxgb\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01moptuna\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01moptuna\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mintegration\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m WandbCallback \u001b[38;5;28;01mas\u001b[39;00m OptunaWandbCallback\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01moptuna\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mintegration\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m XGBoostPruningCallback\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'WandbCallback' from 'optuna.integration' (unknown location)"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "import math\n",
    "from pathlib import Path\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Add the directory containing utils.py to the Python path\n",
    "sys.path.append('.')\n",
    "\n",
    "from typing import Callable\n",
    "\n",
    "import polars as pl\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "import optuna\n",
    "from optuna.integration import WandbCallback as OptunaWandbCallback\n",
    "from optuna.integration import XGBoostPruningCallback\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.compose import TransformedTargetRegressor\n",
    "import shap\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "\n",
    "import wandb\n",
    "from wandb.integration.xgboost import WandbCallback\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Import utility functions\n",
    "import utils\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "DEBUG = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# Read the CSV files\n",
    "data_dir = Path('input-data')\n",
    "thp_vol = pl.read_csv(data_dir / 'traffic_DLThpVol.csv')  # This is the target variable\n",
    "prb = pl.read_csv(data_dir / 'traffic_DLPRB.csv')\n",
    "thp_time = pl.read_csv(data_dir / 'traffic_DLThpTime.csv')\n",
    "mr_number = pl.read_csv(data_dir / 'traffic_MR_number.csv')\n",
    "\n",
    "target_dataframes = {\n",
    "    'thp_vol': thp_vol,\n",
    "    'prb': prb,\n",
    "    'thp_time': thp_time,\n",
    "    'mr_number': mr_number\n",
    "}\n",
    "\n",
    "idx_hour_series = thp_vol['']\n",
    "\n",
    "for k, v in target_dataframes.items():\n",
    "    # Drop the first column (idx hour)\n",
    "    target_dataframes[k] = v.drop('')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "xgb_hyperparams = {\n",
    "    'objective': 'reg:squarederror',\n",
    "    'eval_metric': 'mae',\n",
    "    # 'max_depth': 6,\n",
    "    'eta': 0.1,\n",
    "    'subsample': 0.7,\n",
    "    # 'colsample_bytree': 0.8,\n",
    "    # 'verbosity': 2,\n",
    "    'early_stopping_rounds': 10,\n",
    "    'n_estimators': 100,\n",
    "}\n",
    "\n",
    "config = {\n",
    "    'lags': [1, 2, 3, 6, 12, 13, 14, 24, 25, 26, 48, 49, 72],\n",
    "    'rolling_avgs': [1, 3, 9, 24, 48, 72, 86],\n",
    "    'delta_reference_points': [(1, 2), (1, 3), (1, 6), (1, 24), (24, 25), (48, 49)],\n",
    "    'std_windows': [3, 6, 12, 24, 48, 72, 86],\n",
    "    'num_zeros_windows': [6, 12, 24],\n",
    "    'hour_shifts': [0, 6, 12, 18],\n",
    "    'weekday_shifts': [0, 3, 6],\n",
    "    'train_percentage': 0.6,\n",
    "    'val_percentage': 0.3,  # The rest is test\n",
    "    'run_shap': False,\n",
    "    'target_df_names': [  # dataframes used as target variables\n",
    "        'thp_vol', \n",
    "        'mr_number',\n",
    "        # 'vol_per_prb',\n",
    "    ],\n",
    "    'feat_base_df_names': [  # dataframes used to create the features\n",
    "        'thp_vol', \n",
    "        'mr_number',\n",
    "        'vol_per_user', \n",
    "        # 'vol_per_prb',\n",
    "    ],\n",
    "}\n",
    "\n",
    "if DEBUG:\n",
    "    target_dataframes = {k: v.head(400).select(v.columns[:800]) for k, v in target_dataframes.items()}\n",
    "    # Shorten every list in config to max three elements\n",
    "    config = {k: v[:3] if isinstance(v, list) else v for k, v in config.items()}\n",
    "\n",
    "config = {**xgb_hyperparams, **config}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# Initialize W&B\n",
    "run = wandb.init(\n",
    "    project=\"traffic-forecasting-challenge\", \n",
    "    job_type='train', \n",
    "    entity=\"esedx12\", \n",
    "    config=config, \n",
    "    save_code=True, \n",
    "    mode=('dryrun' if DEBUG else 'online')\n",
    ")\n",
    "\n",
    "# Save utils.py to W&B\n",
    "utils_path = Path('utils.py')\n",
    "if utils_path.exists():\n",
    "    wandb.save(str(utils_path))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Create Interactions Between Targets\n",
    "\n",
    " The following cell is now handled by utility functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# Interaction dataframes are created within utility functions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Feature Engineering\n",
    "\n",
    " The feature engineering steps are handled by utility functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# Use first config.train_percentage of dataframe rows for training, and the rest for validation and testing\n",
    "num_rows = len(target_dataframes['thp_vol'])\n",
    "num_train_rows = round(num_rows * wandb.config.train_percentage)\n",
    "num_val_rows = round(num_rows * wandb.config.val_percentage)\n",
    "\n",
    "train_dataframes = {k: v.head(num_train_rows) for k, v in target_dataframes.items()}\n",
    "train_idx_hour_series = idx_hour_series.head(num_train_rows)\n",
    "\n",
    "val_dataframes = {k: v.slice(num_train_rows + 1, num_val_rows) for k, v in target_dataframes.items()}\n",
    "val_idx_hour_series = idx_hour_series.slice(num_train_rows + 1, num_val_rows)\n",
    "\n",
    "# Create long format dataframes using utility functions\n",
    "long_train_df = utils.create_long_format_df(train_dataframes, train_idx_hour_series, wandb.config)\n",
    "long_val_df = utils.create_long_format_df(val_dataframes, val_idx_hour_series, wandb.config)\n",
    "\n",
    "dropped_cols = ['idx_hour', 'beam_id']\n",
    "target_cols = list(target_dataframes.keys())\n",
    "\n",
    "X_train, y_train = long_train_df.drop(dropped_cols + target_cols), long_train_df.select(target_cols)\n",
    "X_val, y_val = long_val_df.drop(dropped_cols + target_cols), long_val_df.select(target_cols)\n",
    "\n",
    "wandb.config.update({\n",
    "    'train_shape': X_train.shape, \n",
    "    'val_shape': X_val.shape, \n",
    "    'features': X_train.columns, \n",
    "    'targets': y_train.columns\n",
    "})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Train Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "models = {}\n",
    "for target_name in y_train.columns:\n",
    "    model = xgb.XGBRegressor(**xgb_hyperparams, callbacks=[WandbCallback(log_model=True)])\n",
    "    print(f\"\\nFitting model for {target_name}:\")\n",
    "    model.fit(\n",
    "        X_train.to_pandas(), \n",
    "        y_train[target_name].to_pandas(), \n",
    "        eval_set=[(X_train.to_pandas(), y_train[target_name].to_pandas()), \n",
    "                  (X_val.to_pandas(), y_val[target_name].to_pandas())], \n",
    "        verbose=25\n",
    "    )\n",
    "    models[target_name] = model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# Save models\n",
    "for target_name, model in models.items():\n",
    "    model_dir = Path('checkpoints') / wandb.run.name\n",
    "    model_dir.mkdir(parents=True, exist_ok=True)\n",
    "    model_path = model_dir / f'{target_name}.json'  # Changed to .json for XGBoost\n",
    "    model.save_model(model_path)\n",
    "    wandb.save(str(model_path))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Evaluation and Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# Iterate through each model in models\n",
    "for target, target_model in models.items():\n",
    "    print(f\"Processing target: {target}\")\n",
    "\n",
    "    # Predict\n",
    "    train_preds = target_model.predict(X_train.to_pandas())\n",
    "    val_preds = target_model.predict(X_val.to_pandas())\n",
    "\n",
    "    # Compute MAE values\n",
    "    train_mae = mean_absolute_error(y_train[target].to_pandas(), train_preds)\n",
    "    val_mae = mean_absolute_error(y_val[target].to_pandas(), val_preds)\n",
    "\n",
    "    # Log the best score to wandb\n",
    "    # XGBoost does not have best_iteration attribute in scikit-learn API\n",
    "    evals_result = target_model.evals_result()\n",
    "    best_iteration = len(evals_result['validation_0']['mae'])  # Last iteration\n",
    "    best_val_mae = evals_result['validation_1']['mae'][-1]\n",
    "    best_train_mae = evals_result['validation_0']['mae'][-1]\n",
    "\n",
    "    wandb.log({\n",
    "        f'{target}_best_val_mae': best_val_mae, \n",
    "        f'{target}_best_round': best_iteration, \n",
    "        f'{target}_best_train_mae': best_train_mae\n",
    "    })\n",
    "\n",
    "    # Convert evaluation results to a DataFrame\n",
    "    eval_df = pl.DataFrame({\n",
    "        'Round': list(range(1, len(evals_result['validation_0']['mae']) + 1)),\n",
    "        'Train MAE': evals_result['validation_0']['mae'],\n",
    "        'Val MAE': evals_result['validation_1']['mae']\n",
    "    })\n",
    "\n",
    "    # Log eval_df to wandb\n",
    "    wandb.log({f'{target}_eval_df': wandb.Table(data=eval_df.to_pandas())})\n",
    "\n",
    "    # Plot the results using Plotly\n",
    "    fig = px.line(\n",
    "        eval_df.to_pandas(), \n",
    "        x='Round', \n",
    "        y=['Train MAE', 'Val MAE'],\n",
    "        labels={'value': 'Mean Absolute Error'}, \n",
    "        title=f'Training and Validation MAE over Boosting Rounds for {target}'\n",
    "    )\n",
    "    \n",
    "    fig.update_layout(\n",
    "        legend=dict(\n",
    "            title='Dataset',\n",
    "            itemsizing='constant'\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Log the plot to wandb\n",
    "    wandb.log({f\"{target}_MAE_Plot\": fig})\n",
    "\n",
    "    # Optionally, display the plot\n",
    "    fig.show()\n",
    "\n",
    "    print(f\"Best Val MAE for {target}: {best_val_mae}\")\n",
    "    print(f\"Round: {best_iteration}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "if wandb.config.run_shap:\n",
    "    # Create a SHAP explainer for the XGBoost model\n",
    "    # Assuming 'thp_vol' is one of the targets\n",
    "    target_name = 'thp_vol'\n",
    "    explainer = shap.TreeExplainer(models[target_name], X_val.to_pandas())\n",
    "    \n",
    "    # Calculate SHAP values for the val set\n",
    "    shap_values = explainer.shap_values(X_val.to_pandas())\n",
    "    \n",
    "    # Log SHAP plots to wandb\n",
    "    shap_bar = shap.summary_plot(shap_values, X_val.to_pandas(), plot_type=\"bar\", show=False)\n",
    "    plt.savefig(\"shap_bar.png\")\n",
    "    wandb.log({\"SHAP Bar Plot\": wandb.Image(\"shap_bar.png\")})\n",
    "    plt.clf()\n",
    "    \n",
    "    shap_summary = shap.summary_plot(shap_values, X_val.to_pandas(), show=False)\n",
    "    plt.savefig(\"shap_summary.png\")\n",
    "    wandb.log({\"SHAP Summary Plot\": wandb.Image(\"shap_summary.png\")})\n",
    "    plt.clf()\n",
    "\n",
    "    # Optionally, add more SHAP plots as needed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "wandb.finish()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
