{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "os.environ[\"WANDB_NOTEBOOK_NAME\"] = \"xgboost_inference.ipynb\"  # Manually set the notebook name\n",
    "\n",
    "import polars as pl\n",
    "import xgboost as xgb\n",
    "import wandb\n",
    "from tqdm.notebook import tqdm\n",
    "import pickle\n",
    "\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEBUG = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose training run from which to load the model, etc.\n",
    "train_run_name = 'prime-wind-87'\n",
    "run_path = f'esedx12/traffic-forecasting-challenge/{train_run_name}'\n",
    "api = wandb.Api()\n",
    "train_run = api.runs(\n",
    "    path=\"esedx12/traffic-forecasting-challenge\",\n",
    "    filters={\"display_name\": {\"$eq\": train_run_name}} \n",
    ")[0]\n",
    "train_config = train_run.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_config = {\n",
    "    # 'prediction_start': train_config['train_shape'][0] + 1,\n",
    "    'prediction_length': 10,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "W&B syncing is set to <code>`offline`<code> in this directory.  <br/>Run <code>`wandb online`<code> or set <code>WANDB_MODE=online<code> to enable cloud syncing."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run = wandb.init(project=\"traffic-forecasting-challenge\", tags=[train_run_name], job_type='inference',\n",
    "                 entity=\"esedx12\", config=inference_config, save_code=True, mode=('dryrun' if DEBUG else 'online'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoints_dir = 'checkpoints'\n",
    "xgboost_models_dir = Path(checkpoints_dir) / train_run_name\n",
    "\n",
    "models = {}\n",
    "for file_name in os.listdir(xgboost_models_dir):\n",
    "    if file_name.endswith('.ubj'):\n",
    "        target_name = file_name[:-4]\n",
    "        model_path = xgboost_models_dir / file_name\n",
    "        models[target_name] = pickle.load(open(model_path, 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# Read the CSV files\n",
    "data_dir = Path('input-data')\n",
    "target_dataframes = {\n",
    "    'thp_vol': pl.read_csv(data_dir / 'traffic_DLThpVol.csv'),  # This is the target variable\n",
    "    'prb': pl.read_csv(data_dir / 'traffic_DLPRB.csv'),\n",
    "    'thp_time': pl.read_csv(data_dir / 'traffic_DLThpTime.csv'),\n",
    "    'mr_number': pl.read_csv(data_dir / 'traffic_MR_number.csv')\n",
    "}\n",
    "\n",
    "# Filter target dataframes based on train_config\n",
    "target_dataframes = {k: v for k, v in target_dataframes.items() if k in train_config['target_df_names']}\n",
    "\n",
    "idx_hour_series = target_dataframes['thp_vol']['']\n",
    "\n",
    "# Drop the first column (idx hour) from each dataframe\n",
    "for k in target_dataframes:\n",
    "    target_dataframes[k] = target_dataframes[k].drop('')\n",
    "\n",
    "# A long format beam_id column to be used for converting to wide format\n",
    "beam_id_col = utils.convert_to_long_format({'beam_id': pl.DataFrame({beam_id: [beam_id] * len(target_dataframes['thp_vol']) for beam_id in target_dataframes['thp_vol'].columns})})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_rows = len(target_dataframes['thp_vol'])\n",
    "num_train_rows = round(num_rows * train_config['train_percentage'])\n",
    "# num_val_rows = round(num_rows * train_config['val_percentage'])\n",
    "\n",
    "# Split data into train and test\n",
    "input_dataframes = {k: v.head(num_train_rows) for k, v in target_dataframes.items()}\n",
    "comparison_dataframes = {k: v.slice(num_train_rows, inference_config['prediction_length']) for k, v in target_dataframes.items()}\n",
    "# TODO add different df sets form idx of validation and holdout test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Step Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_one_step(target_dataframes: dict[pl.DataFrame], idx_hour_series: pl.Series ,models: xgb.Booster, train_config: wandb.Config) -> dict[pl.DataFrame]:\n",
    "    \"\"\"\n",
    "    Predict one step into the future using a trained model.\n",
    "    Takes DataFrames of len n, returns DataFrames of len n + 1.\n",
    "    \"\"\"\n",
    "    template_df = target_dataframes['thp_vol']\n",
    "    predict_hour = idx_hour_series[-1] + 1\n",
    "\n",
    "    null_row = pl.DataFrame({beam_id: [None] for beam_id in template_df.columns})\n",
    "    target_dataframes = {k: pl.concat([v, null_row], how='vertical_relaxed') for k, v in target_dataframes.items()}\n",
    "\n",
    "    target_names = list(target_dataframes.keys())\n",
    "    feature_dfs = utils.create_all_feature_dfs(target_dataframes, idx_hour_series, train_config)\n",
    "    feature_dfs = {k: v.tail(1) for k, v in feature_dfs.items()}  # maybe turn in to lazyframe for efficiency?\n",
    "    X_predict = utils.convert_to_long_format(feature_dfs)\n",
    "\n",
    "    # We predict only the idx immediately folling the last idx in the input, ie a single row\n",
    "    ys_predicted_long = pl.DataFrame()\n",
    "    for target_name, model in models.items():\n",
    "        y_predicted = model.predict(X_predict.to_numpy())\n",
    "        ys_predicted_long = pl.concat([ys_predicted_long, pl.DataFrame({target_name: y_predicted})], how='horizontal')\n",
    "\n",
    "    # We need these long-format columns to convert the predictions to wide format\n",
    "    util_dfs = {}\n",
    "    util_dfs['beam_id'] = pl.DataFrame({beam_id: [beam_id] for beam_id in template_df.columns})\n",
    "    util_dfs['idx_hour'] = pl.DataFrame({beam_id: [predict_hour] for beam_id in template_df.columns})\n",
    "    util_long_df = utils.convert_to_long_format(util_dfs)\n",
    "    ys_predicted_long = pl.concat([util_long_df, ys_predicted_long], how='horizontal')\n",
    "\n",
    "    y_predicted_wide = utils.convert_to_wide_format(ys_predicted_long, output_df_names=target_names)\n",
    "\n",
    "    return (\n",
    "        {target_name: pl.concat([target_dataframes[target_name].head(-1), y_predicted_wide[target_name]], how='vertical_relaxed') for target_name in target_names},\n",
    "        idx_hour_series.append(pl.Series([predict_hour]))\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_multi_step(target_dataframes: dict[pl.DataFrame], idx_hour_series: pl.Series, models: xgb.Booster, train_config: wandb.Config, num_steps: int, max_lag=None) -> dict[pl.DataFrame]:\n",
    "    \"\"\"\n",
    "    Predict multiple steps into the future using a trained model.\n",
    "    Takes DataFrames of len n, returns DataFrames of len n + num_steps.\n",
    "    \n",
    "    Args:\n",
    "        target_dataframes (dict): A dictionary of DataFrames representing the target data.\n",
    "        idx_hour_series (Series): Index hours CORRESPONDING to target_dataframes.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary of DataFrames representing the predicted target dataframes.\n",
    "    \"\"\"\n",
    "    if max_lag:\n",
    "        target_dataframes = {k: v.tail(max_lag + 5) for k, v in target_dataframes.items()}\n",
    "        idx_hour_series = idx_hour_series.tail(max_lag + 5)\n",
    "\n",
    "    for _ in tqdm(range(num_steps), desc='Predicting steps...'):\n",
    "        target_dataframes, idx_hour_series = predict_one_step(target_dataframes, idx_hour_series, models, train_config)\n",
    "\n",
    "    return target_dataframes, idx_hour_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "008d74a3941f4848a7c6173351191067",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting steps...:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ys_pred, idx_hour_series = predict_multi_step(input_dataframes, idx_hour_series, models, train_config=train_config, num_steps=inference_config['prediction_length'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_absolute_error(Y_true: pl.DataFrame, Y_pred: pl.DataFrame) -> float:\n",
    "    \"\"\"\n",
    "    Compute the mean absolute error between two DataFrames.\n",
    "    \"\"\"\n",
    "    # assert (Y_true['idx_hour'] == Y_pred['idx_hour']).all(), \"DataFrames must be aligned\"\n",
    "    assert Y_true.shape == Y_pred.shape, \"DataFrames must have the same shape\"\n",
    "\n",
    "    return (Y_true - Y_pred).select(pl.all().abs().mean()).mean_horizontal()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.20510621259602663"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_absolute_error(comparison_dataframes['thp_vol'], ys_pred['thp_vol'].tail(inference_config['prediction_length']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ...on Validation Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ...on Test Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ...on Validation and Test Sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Submission CSV\n",
    "\n",
    "* Hours in 5 weeks: 840\n",
    "* Hours in 6 weeks: 1008\n",
    "* We need period 841-1008 (841-1009 with Python list indexing)\n",
    "\n",
    "* Hours in 10 weeks: 1680\n",
    "* Hours in 11 weeks: 1848"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_half_submission_df(input_df: pl.DataFrame, weeks: str) -> pl.DataFrame:\n",
    "    \"\"\"\n",
    "    Create a submission CSV file from a Polars DataFrame.\n",
    "    \"\"\"\n",
    "    if weeks == '5w-6w':\n",
    "        range = [841, 1008]\n",
    "    elif weeks == '10w-11w':\n",
    "        range = [1681, 1848]\n",
    "\n",
    "    # Choose rows with first column 'idx_hour' having the values 671-840.\n",
    "    input_df = input_df.filter(pl.col('idx_hour').is_in(range)).with_row_index()\n",
    "\n",
    "    # Check that shape of dataframe is (168, 2881)\n",
    "    assert input_df.shape == (168, 2881), f\"Expected shape (168, 2881), got {input_df.shape}\"\n",
    "\n",
    "    # Check that there is no null value in the dataframe\n",
    "    assert input_df.is_null().any().any() == False, \"Submission dataframe contains null values\"\n",
    "\n",
    "    # Stack the dataframe with f'traffic_DLThpVol_test_5w-6w_{hour}_{beam_id}' as index\n",
    "    # where it cycles through the values 671-840 for hour and then the beam_ids, which are colnames of input_df\n",
    "    return input_df.unpivot(index='idx_hour').with_columns(\n",
    "        (pl.struct(pl.all()).map_elements(lambda row: f'traffic_DLThpVol_test_5w-6w_{row['row_index']}_{row[\"variable\"]}', return_dtype=pl.String)).alias('ID')\n",
    "    ).select(['ID', 'value']).rename({'value': 'Target'})\n",
    "\n",
    "\n",
    "def create_submission_csv(input_df: pl.DataFrame, output_filename='traffic_forecast.csv', archiving_dir='submission-csvs-archive') -> pl.DataFrame:\n",
    "    \"\"\"\n",
    "    Create a submission CSV file from data in input format that's been extended to cover weeks 5-6 and 10-11.\n",
    "    \"\"\"\n",
    "\n",
    "    # Create half submission dataframes\n",
    "    half_submission_5w_6w = create_half_submission_df(input_df, '5w-6w')\n",
    "    half_submission_10w_11w = create_half_submission_df(input_df, '10w-11w')\n",
    "\n",
    "    # Concatenate the two half submission dataframes\n",
    "    submission_df = pl.concat([half_submission_5w_6w, half_submission_10w_11w], how='vertical')\n",
    "\n",
    "    # Save the submission dataframe to a CSV file for submission\n",
    "    submission_df.write_csv(output_filename)\n",
    "    \n",
    "    # Save the submission dataframe to a CSV file for archiving\n",
    "    if archiving_dir:\n",
    "        archiving_dir = Path(archiving_dir)\n",
    "        archiving_dir.mkdir(parents=True, exist_ok=True)\n",
    "        submission_df.write_csv(archiving_dir / f'{wandb.run.name}_{output_filename}')\n",
    "\n",
    "    return submission_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
